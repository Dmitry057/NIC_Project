%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[aspectratio=169,xcolor=dvipsnames]{beamer}
\usetheme{SimpleDarkBlue}

\usepackage{hyperref}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{makecell} % Allows the use of \makecell for table cells

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Fraud Transaction Detection: Project Report}
\subtitle{Nature Inspired Computing}

\author{Nikita Zagainov, Dmitry Tetkin, Alisher Kamolov, Nikita Tsukanov}

\institute
{
    Innopolis University
}
\date{April 2025} % Date, can be changed to a custom date

%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{document}

\begin{frame}
	% Print the title page as the first slide
	\titlepage
\end{frame}


\begin{frame}{Project Description}
	Application of nature-inspired algorithms
	to the problem of fraud transaction detection
	in banking systems.
	\begin{itemize}
		\item Fraud detection is a critical task in banking systems
		\item Most common approach is Machine Learning
		\item We propose to use nature-inspired algorithms to fine-tune
		      ML models to get better results
	\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Reference Work}
	Our project was inspired by the following project:
	\url{https://github.com/pmacinec/transactions-fraud-detection}
	Our project's contribution is the following:
	\begin{itemize}
		\item Comparison with gradient boosting algorithms
		\item Application of NIC algorithms to the gradient boosting
		      algorithms
	\end{itemize}

\end{frame}

\begin{frame}{Results}
	\begin{table}
		\centering
		\caption{ROC AUC of ML models with NIC feature selection}
		\label{tab:results}
		\begin{tabular}{c c c c}
			\hline
			Method                & CatBoost       & LightGBM       & DecisionTree   \\
			\hline
			Original Score        & \textbf{0.893} & 0.909          & 0.835          \\
			Artificial Bee Colony & 0.887          & 0.906          & 0.836          \\
			Cuckoo Search         & 0.891          & 0.905          & 0.842          \\
			Bat Algorithm         & 0.889          & 0.909          & 0.842          \\
			Firefly Algorithm     & 0.892          & 0.907          & 0.844          \\
			Flower Pollination    & 0.891          & 0.906          & \textbf{0.846} \\
			Grey Wolf Optimizer   & 0.892          & \textbf{0.913} & 0.844          \\
			Particle Swarm        & 0.892          & 0.912          & 0.845          \\
			\hline
			\makecell{Algorithms better                                              \\
			than baseline}        & 0              & 2              & 7              \\
			\hline
		\end{tabular}
	\end{table}
\end{frame}

%------------------------------------------------

\begin{frame}{Conclusion}
	\begin{itemize}
		\item NIC algorithms for feature selection do not improve
		      performance of gradient boosting models
		\item We hypothesize that the reason for such difference
		      in results between single tree and ensemble is that
		      single tree is more prone to overfitting
	\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
	\Huge{\centerline{\textbf{Thank you for your attention!}}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}